theme_bw() + scale_color_discrete(name="predictors", labels=names(training_set))
lm.fit <- lm(lpsa~., data=training_set)
coef_sum <- sum(lm.fit$coefficients)
fit <- glmnet(as.matrix(training_set[,1:p]), training_set[,p+1],
family="gaussian", alpha=1)
lasso.fit <- glmnet(as.matrix(training_set[,1:p]), training_set[,p+1],
family="gaussian", alpha=1,
lambda=seq(from=min(fit$lambda), to=max(fit$lambda), length=1000))
lambda <- lasso.fit$lambda
beta <- lasso.fit$beta
s <- apply(beta, 2, sum)/coef_sum
num <- length(lambda)
for(i in 1:num) {
if(i == 1) {
ShrinkageFactor <- as.matrix(rep(s[i], p))
classRes <- as.matrix(1:p)
betaRes <- as.matrix(beta[,i])
} else {
ShrinkageFactor <- rbind(ShrinkageFactor, as.matrix(rep(s[i],p)))
classRes <- rbind(classRes, as.matrix(1:p))
betaRes <- rbind(betaRes, as.matrix(beta[,i]))
}
}
### Plot Figure 3.7
plotDf <- data.frame(cbind(classRes,betaRes,ShrinkageFactor))
names(plotDf) <- c("class","beta","s")
plotDf$class <- as.factor(plotDf$class)
library(ggplot2)
ggplot(data=plotDf, aes(x=s,y=beta,col=class)) + geom_line() +
labs(title="Figure 3.10:",
x="Shrinkage Factor s", y="Coefficients (Lasso)") +
theme_bw() + scale_color_discrete(name="predictors", labels=names(training_set))
lm.fit$coefficients
coef_sum <- sum(abs(lm.fit$coefficients))
coef_sum
s <- apply(abs(beta), 2, sum)/coef_sum
s
lm.fit <- lm(lpsa~., data=training_set)
coef_sum <- sum(abs(lm.fit$coefficients))
fit <- glmnet(as.matrix(training_set[,1:p]), training_set[,p+1],
family="gaussian", alpha=1)
lasso.fit <- glmnet(as.matrix(training_set[,1:p]), training_set[,p+1],
family="gaussian", alpha=1,
lambda=seq(from=min(fit$lambda), to=max(fit$lambda), length=1000))
lambda <- lasso.fit$lambda
beta <- lasso.fit$beta
s <- apply(abs(beta), 2, sum)/coef_sum
num <- length(lambda)
for(i in 1:num) {
if(i == 1) {
ShrinkageFactor <- as.matrix(rep(s[i], p))
classRes <- as.matrix(1:p)
betaRes <- as.matrix(beta[,i])
} else {
ShrinkageFactor <- rbind(ShrinkageFactor, as.matrix(rep(s[i],p)))
classRes <- rbind(classRes, as.matrix(1:p))
betaRes <- rbind(betaRes, as.matrix(beta[,i]))
}
}
### Plot Figure 3.7
plotDf <- data.frame(cbind(classRes,betaRes,ShrinkageFactor))
names(plotDf) <- c("class","beta","s")
plotDf$class <- as.factor(plotDf$class)
library(ggplot2)
ggplot(data=plotDf, aes(x=s,y=beta,col=class)) + geom_line() +
labs(title="Figure 3.10:",
x="Shrinkage Factor s", y="Coefficients (Lasso)") +
theme_bw() + scale_color_discrete(name="predictors", labels=names(training_set))
lm.fit <- lm(lpsa~., data=training_set)
coef_sum <- sum(abs(lm.fit$coefficients))
lasso.fit <- glmnet(as.matrix(training_set[,1:p]), training_set[,p+1],
family="gaussian", alpha=1)
#lasso.fit <- glmnet(as.matrix(training_set[,1:p]), training_set[,p+1],
#                    family="gaussian", alpha=1,
#                    lambda=seq(from=min(fit$lambda), to=max(fit$lambda), length=1000))
lambda <- lasso.fit$lambda
beta <- lasso.fit$beta
s <- apply(abs(beta), 2, sum)/coef_sum
num <- length(lambda)
for(i in 1:num) {
if(i == 1) {
ShrinkageFactor <- as.matrix(rep(s[i], p))
classRes <- as.matrix(1:p)
betaRes <- as.matrix(beta[,i])
} else {
ShrinkageFactor <- rbind(ShrinkageFactor, as.matrix(rep(s[i],p)))
classRes <- rbind(classRes, as.matrix(1:p))
betaRes <- rbind(betaRes, as.matrix(beta[,i]))
}
}
### Plot Figure 3.7
plotDf <- data.frame(cbind(classRes,betaRes,ShrinkageFactor))
names(plotDf) <- c("class","beta","s")
plotDf$class <- as.factor(plotDf$class)
library(ggplot2)
ggplot(data=plotDf, aes(x=s,y=beta,col=class)) + geom_line() +
labs(title="Figure 3.10:",
x="Shrinkage Factor s", y="Coefficients (Lasso)") +
theme_bw() + scale_color_discrete(name="predictors", labels=names(training_set))
ggplot(data=plotDf, aes(x=s,y=beta,col=class)) + geom_line() +
labs(title="Figure 3.10:",
x="Shrinkage Factor s", y="Coefficients (Lasso)") +
theme_bw() + scale_color_discrete(name="predictors", labels=names(training_set)) +
geom_vline(aes(xintercept=0.36, alpha=0.5), color="red", linetype="dashed", size=1)
ggplot(data=plotDf, aes(x=s,y=beta,col=class)) + geom_line() +
labs(title="Figure 3.10:",
x="Shrinkage Factor s", y="Coefficients (Lasso)") +
theme_bw() + scale_color_discrete(name="predictors", labels=names(training_set)) +
geom_vline(aes(xintercept=0.365, alpha=0.5), color="red", linetype="dashed", size=1)
ggplot(data=plotDf, aes(x=s,y=beta,col=class)) + geom_line() +
labs(title="Figure 3.10:",
x="Shrinkage Factor s", y="Coefficients (Lasso)") +
theme_bw() + scale_color_discrete(name="predictors", labels=names(training_set)) +
geom_vline(aes(xintercept=0.368, alpha=0.5), color="red", linetype="dashed", size=1)
ggplot(data=plotDf, aes(x=s,y=beta,col=class)) + geom_line() +
labs(title="Figure 3.10:",
x="Shrinkage Factor s", y="Coefficients (Lasso)") +
theme_bw() + scale_color_discrete(name="predictors", labels=names(training_set)) +
geom_vline(aes(xintercept=0.37, alpha=0.5), color="red", linetype="dashed", size=1)
ggplot(data=plotDf, aes(x=s,y=beta,col=class)) + geom_line() +
labs(title="Figure 3.10: Profiles of lasso coefficients for the prostate cancer example",
x="Shrinkage Factor s", y="Coefficients (Lasso)") +
theme_bw() + scale_color_discrete(name="predictors", labels=names(training_set)) +
geom_vline(aes(xintercept=0.37, alpha=0.5), color="red", linetype="dashed", size=1)
install.packages("mxnet", repo="http://104.131.113.222")
?matrix
A <- matrix(c(1,1,1,1,0,1,3,4), nrow=4, ncol=2)
A
t(A) * A
t(A) %*% A
?inv
library(mass)
library(Mass)
library(MASS)
?inv
?ginv
b <- vector(c(0,8,8,20))
?vector
b <- as.vector(c(0,8,8,20))
b
t(A) %*% b
p <- A %*% ginv(t(A) %*% A) %*% t(A) %*% b
p
e <- b-p
e
E <- sum(e^2)
E
t(e) %*% A[,1]
t(e) %*% A[,2]
A[,1]
A[,2]
require(quantmod)
getSymbols.oanda("USD/EUR", from="2016-01-01")
getSymbols("USD/EUR", src="oanda", from="2016-01-01")
USDEUR
getSymbols("CNY/CAD", src="oanda", from="2016-01-01")
CNYCAD
getSymbols("CAD/CNY", src="oanda", from="2016-01-01")
CADCNY
require(quantmod)
rate <- getSymbols("CAD/CNY",src="ondoa",from="2016-01-01")
rate <- getSymbols("CAD/CNY",src="oanda",from="2016-01-01")
rate
str(rate)
CADCNY
rm(rate)
getSymbols("USD/CNY",src="oanda",from="2016-01-01")
USDCNY
plot(USDCNY)
plot(CADCNY)
getSymbols("USD/CAD",src="oanda",from="2016-01-01")
plot(USDCAD)
rate <- getSymbols("CAD/CNY",src="oanda",from="2015-01-01")
rm(rate)
CADCNY
min(CADCNY)
plot(CADCNY)
getSymbols("USD/CNY",src="oanda",from="2015-01-01")
plot(USDCNY)
getSymbols("USD/CAD",src="oanda",from="2015-01-01")
plot(USDCAD)
plot(CADCNY)
getSymbols("CAD/CNY",src="oanda",from="2015-01-01")
plot(CADCNY)
CADCNY
getSymbols("CAD/CNY",src="oanda",from="2015-01-01")
plot(CADCNY)
CADCNY
getSymbols("CAD/CNY",src="oanda",from="2015-01-01")
plot(CADCNY)
CADCNY
require(quantmod)
getSymbols('CADCNY', src="oanda", from=2016-01-01)
getSymbols('CADCNY', src="oanda", from="2016-01-01")
getSymbols('CAD/CNY', src="oanda", from="2016-01-01")
plot(CADCNY)
CADCNY
getSymbols('CAD/CNY', src="oanda", from="2016-01-01")
require('quantmod')
getSymbols('CAD/CNY', src="oanda", from="2016-01-01")
plot(CADCNY)
getSymbols('CAD/CNY', src="oanda", from="2016-01-01")
require('quantmod')
getSymbols('CAD/CNY', src="oanda", from="2016-01-01")
plot(CADCNY)
setwd("~/Coding/R/CMPUT466/mlclass-ex5")
library(ggplot2)
library(R.matlab)
library(devtools)
source_url("https://raw.githubusercontent.com/ggrothendieck/gsubfn/master/R/list.R")
data <- readMat("ex5data1.mat")
X <- data$X
y <- data$y
Xtest <- data$Xtest
ytest <- data$ytest
Xval <- data$Xval
yval <- data$yval
df <- data.frame(X=X, y=y)
ggplot(df, aes(X, y, col="Training data")) + geom_point(shape=I(4), size=I(3)) +
labs(x="Change in water level (x)", y="Water flowing out of the damn (y)") +
scale_color_manual(guide=FALSE, values=c("Red"))
computeCost = function(theta, X, y, lambda) {
m = length(y)
n = length(theta)
sum((X %*% theta - y)^2)/(2*m) + lambda/(2*m)*t(theta[2:n]) %*% theta[2:n]
}
myTheta <- c(1., 1.)
computeCost(myTheta, cbind(1, X), y, 1)
computeGrad = function(theta, X, y, lambda) {
m = length(y)
n = length(theta)
tmp <- theta
tmp[1] = 0
1./m*t(X) %*% (X %*% theta - y) + lambda/m*tmp
}
computeGrad(myTheta, cbind(1, X), y, 1)
trainLinearReg <- function(X, y, lambda, maxit) {
n <- dim(X)[2]
theta <- matrix(0, n, 1)
res <- optim(theta, computeCost, computeGrad, X, y, lambda, method = "CG", control = list(maxit = maxit))
res$par
}
lambda <- 0
theta <- trainLinearReg(cbind(1,X), y, lambda, 10000)
df$y.pred <- cbind(1, X) %*% theta
ggplot(df, aes(X, y, col="Training data")) + geom_point(shape=I(4), size=I(3)) +
labs(x="Change in water level (x)", y="Water flowing out of the damn (y)", title="Linear Regression Fit") +
geom_line(aes(X, y.pred, col="Regularized linear regression")) +
scale_color_manual(guide=FALSE, values=c("Blue", "Red"))
learningCurve <- function(X, y, X_val, y_val, lambda, maxit) {
m = length(y)
error.train = matrix(0, m, 1)
error.valid = matrix(0, m, 1)
for(i in 1:m) {
theta <- trainLinearReg(X[1:i,,drop=FALSE], y[1:i], lambda, maxit)
error.train[i] = computeCost(theta, X[1:i,], y[1:i], 0)
error.valid[i] = computeCost(theta, X_val, y_val, 0)
}
list(error.train=error.train, error.valid=error.valid)
}
error.train = 0
error.valid = 0
list[error.train, error.valid] <- learningCurve(cbind(1, X), y, cbind(1, Xval), yval, lambda, 10000)
m = length(y)
df_lc <- data.frame(x=1:m, error.train=error.train, error.valid=error.valid)
ggplot(df_lc, aes(x)) + geom_line(aes(y=error.train, col="training error")) +
geom_line(aes(y=error.valid, col="validation error")) +
labs(x="Number of training examples", y="Error", title="Learning curve for linear regression") +
scale_color_discrete(name="")
polyFeatures = function(X, p) {
X_poly = matrix(0, length(X), p)
for(i in 1:p)
X_poly[,i] = X^i
X_poly
}
p <- 8
X_poly <- polyFeatures(X, p)
X_poly <- scale(X_poly)
mu <- attr(X_poly, "scaled:center")
sigma <- attr(X_poly, "scaled:scale")
X_poly <- cbind(1, X_poly)
X_poly_test <- polyFeatures(Xtest, p)
X_poly_test <- sweep(X_poly_test, 2, mu)
X_poly_test <- sweep(X_poly_test, 2, sigma, "/")
X_poly_test <- cbind(1, X_poly_test)
X_poly_val <- polyFeatures(Xval, p)
X_poly_val <- sweep(X_poly_val, 2, mu)
X_poly_val <- sweep(X_poly_val, 2, sigma, "/")
X_poly_val <- cbind(1, X_poly_val)
lambda <- 0
n = dim(X_poly)[2]
theta <- trainLinearReg(X_poly, y, lambda, 200)
idx <- seq(min(X)-15, max(X)+15, length.out=100)
X_poly_fit <- polyFeatures(idx, p)
X_poly_fit <- sweep(X_poly_fit, 2, mu)
X_poly_fit <- sweep(X_poly_fit, 2, sigma, "/")
X_poly_fit <- cbind(1, X_poly_fit)
df_fit <- data.frame(x=idx,y=X_poly_fit %*% theta)
ggplot(df, aes(X, y, col="Training data")) + geom_point(shape=I(4), size=I(3)) +
labs(x="Change in water level (x)", y="Water flowing out of the damn (y)", title=sprintf("Polynomial Regression Fit (lambda = %.1f)", lambda)) +
geom_line(aes(x, y, col="Regularized polynomial regression"), df_fit, linetype="dashed") +
scale_color_manual(guide=FALSE, values=c("Blue", "Red"))
list[error.train, error.valid] <- learningCurve(X_poly, y, X_poly_val, yval, lambda, 200)
df_lc <- data.frame(x=1:m, error.train=error.train, error.valid=error.valid)
ggplot(df_lc, aes(x)) + geom_line(aes(y=error.train, col="training error")) +
geom_line(aes(y=error.valid, col="validation error")) +
labs(x="Number of training examples", y="Error", title=sprintf("Polynomial Regression Learning Curve (lambda = %.1f)", lambda)) +
scale_color_discrete(name="")
validationCurve <- function(X, y, Xval, yval, maxit) {
lambda_vec = c(0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10)
m = length(lambda_vec)
error.train = matrix(0, m, 1)
error.valid = matrix(0, m, 1)
for(i in 1:m) {
lambda = lambda_vec(i)
theta <- trainLinearReg(X, y, lambda, maxit)
error.train[i] = computeCost(theta, X, y, 0)
error.valid[i] = computeCost(theta, Xval, yval, 0)
}
list(lambda_vec=lambda_vec, error.train=error.train, error.valid=error.valid)
}
list[lambda_vec, error.train, error.valid] <- validationCurve(X_poly, y, X_poly_val, yval, 200)
df_cv <- data.frame(lambda_vec=lambda_vec, error.train=error.train, error.valid=error.valid)
ggplot(df_cv, aes(lambda_vec)) + geom_line(aes(y=error.train, col="training error")) +
geom_line(aes(y=error.valid, col="validation error")) +
labs(x="Lambda", y="Error") +
scale_color_discrete(name="")
validationCurve <- function(X, y, Xval, yval, maxit) {
lambda_vec = c(0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10)
m = length(lambda_vec)
error.train = matrix(0, m, 1)
error.valid = matrix(0, m, 1)
for(i in 1:m) {
lambda = lambda_vec(i)
theta <- trainLinearReg(X, y, lambda, maxit)
error.train[i] = computeCost(theta, X, y, 0)
error.valid[i] = computeCost(theta, Xval, yval, 0)
}
list(lambda_vec=lambda_vec, error.train=error.train, error.valid=error.valid)
}
lambda_vec = 0
list[lambda_vec, error.train, error.valid] <- validationCurve(X_poly, y, X_poly_val, yval, 200)
df_cv <- data.frame(lambda_vec=lambda_vec, error.train=error.train, error.valid=error.valid)
ggplot(df_cv, aes(lambda_vec)) + geom_line(aes(y=error.train, col="training error")) +
geom_line(aes(y=error.valid, col="validation error")) +
labs(x="Lambda", y="Error") +
scale_color_discrete(name="")
validationCurve <- function(X, y, Xval, yval, maxit) {
lambda_vec = c(0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10)
m = length(lambda_vec)
error.train = matrix(0, m, 1)
error.valid = matrix(0, m, 1)
for(i in 1:m) {
lambda = lambda_vec(i)
theta <- trainLinearReg(X, y, lambda, maxit)
error.train[i] = computeCost(theta, X, y, 0)
error.valid[i] = computeCost(theta, Xval, yval, 0)
}
list(lambda_vec=lambda_vec, error.train=error.train, error.valid=error.valid)
}
res <- validationCurve(X_poly, y, X_poly_val, yval, 200)
lambda_vec <- res$lambda_vec
error.train <- res$error.train
error.valid <- res$error.valid
df_cv <- data.frame(lambda_vec=lambda_vec, error.train=error.train, error.valid=error.valid)
ggplot(df_cv, aes(lambda_vec)) + geom_line(aes(y=error.train, col="training error")) +
geom_line(aes(y=error.valid, col="validation error")) +
labs(x="Lambda", y="Error") +
scale_color_discrete(name="")
validationCurve <- function(X, y, Xval, yval, maxit) {
lambda_vec = c(0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10)
m = length(lambda_vec)
error.train = matrix(0, m, 1)
error.valid = matrix(0, m, 1)
for(i in 1:m) {
lambda = lambda_vec[i]
theta <- trainLinearReg(X, y, lambda, maxit)
error.train[i] = computeCost(theta, X, y, 0)
error.valid[i] = computeCost(theta, Xval, yval, 0)
}
list(lambda_vec=lambda_vec, error.train=error.train, error.valid=error.valid)
}
res <- validationCurve(X_poly, y, X_poly_val, yval, 200)
lambda_vec <- res$lambda_vec
error.train <- res$error.train
error.valid <- res$error.valid
df_cv <- data.frame(lambda_vec=lambda_vec, error.train=error.train, error.valid=error.valid)
ggplot(df_cv, aes(lambda_vec)) + geom_line(aes(y=error.train, col="training error")) +
geom_line(aes(y=error.valid, col="validation error")) +
labs(x="Lambda", y="Error") +
scale_color_discrete(name="")
validationCurve <- function(X, y, Xval, yval, maxit) {
lambda_vec = c(0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10)
m = length(lambda_vec)
error.train = matrix(0, m, 1)
error.valid = matrix(0, m, 1)
for(i in 1:m) {
lambda = lambda_vec[i]
theta <- trainLinearReg(X, y, lambda, maxit)
error.train[i] = computeCost(theta, X, y, 0)
error.valid[i] = computeCost(theta, Xval, yval, 0)
}
list(lambda_vec=lambda_vec, error.train=error.train, error.valid=error.valid)
}
list[lambda_vec, error.train, error.valid] <- validationCurve(X_poly, y, X_poly_val, yval, 200)
df_cv <- data.frame(lambda_vec=lambda_vec, error.train=error.train, error.valid=error.valid)
ggplot(df_cv, aes(lambda_vec)) + geom_line(aes(y=error.train, col="training error")) +
geom_line(aes(y=error.valid, col="validation error")) +
labs(x="Lambda", y="Error") +
scale_color_discrete(name="")
validationCurve <- function(X, y, Xval, yval, maxit) {
lambda_vec = c(0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10)
m = length(lambda_vec)
error.train = matrix(0, m, 1)
error.valid = matrix(0, m, 1)
for(i in 1:m) {
lambda = lambda_vec[i]
theta <- trainLinearReg(X, y, lambda, maxit)
error.train[i] = computeCost(theta, X, y, 0)
error.valid[i] = computeCost(theta, Xval, yval, 0)
}
list(lambda_vec=lambda_vec, error.train=error.train, error.valid=error.valid)
}
list[lambda_vec, error.train, error.valid] <- validationCurve(X_poly, y, X_poly_val, yval, 10000)
df_cv <- data.frame(lambda_vec=lambda_vec, error.train=error.train, error.valid=error.valid)
ggplot(df_cv, aes(lambda_vec)) + geom_line(aes(y=error.train, col="training error")) +
geom_line(aes(y=error.valid, col="validation error")) +
labs(x="Lambda", y="Error") +
scale_color_discrete(name="")
lambda_cec
lambda_vec
error.train
error.valid
validationCurve <- function(X, y, Xval, yval, maxit) {
lambda_vec = c(0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10)
m = length(lambda_vec)
error.train = matrix(0, m, 1)
error.valid = matrix(0, m, 1)
for(i in 1:m) {
lambda = lambda_vec[i]
theta <- trainLinearReg(X, y, lambda, maxit)
error.train[i] = computeCost(theta, X, y, 0)
error.valid[i] = computeCost(theta, Xval, yval, 0)
}
list(lambda_vec=lambda_vec, error.train=error.train, error.valid=error.valid)
}
list[lambda_vec, error.train, error.valid] <- validationCurve(X_poly, y, X_poly_val, yval, 100)
df_cv <- data.frame(lambda_vec=lambda_vec, error.train=error.train, error.valid=error.valid)
ggplot(df_cv, aes(lambda_vec)) + geom_line(aes(y=error.train, col="training error")) +
geom_line(aes(y=error.valid, col="validation error")) +
labs(x="Lambda", y="Error") +
scale_color_discrete(name="")
validationCurve <- function(X, y, Xval, yval, maxit) {
lambda_vec = c(0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10)
m = length(lambda_vec)
error.train = matrix(0, m, 1)
error.valid = matrix(0, m, 1)
for(i in 1:m) {
lambda = lambda_vec[i]
theta <- trainLinearReg(X, y, lambda, maxit)
error.train[i] = computeCost(theta, X, y, 0)
error.valid[i] = computeCost(theta, Xval, yval, 0)
}
list(lambda_vec=lambda_vec, error.train=error.train, error.valid=error.valid)
}
list[lambda_vec, error.train, error.valid] <- validationCurve(X_poly, y, X_poly_val, yval, 200)
df_cv <- data.frame(lambda_vec=lambda_vec, error.train=error.train, error.valid=error.valid)
ggplot(df_cv, aes(lambda_vec)) + geom_line(aes(y=error.train, col="training error")) +
geom_line(aes(y=error.valid, col="validation error")) +
labs(x="Lambda", y="Error") +
scale_color_discrete(name="")
validationCurve <- function(X, y, Xval, yval, maxit) {
lambda_vec = c(0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10)
m = length(lambda_vec)
error.train = matrix(0, m, 1)
error.valid = matrix(0, m, 1)
for(i in 1:m) {
lambda = lambda_vec[i]
theta <- trainLinearReg(X, y, lambda, maxit)
error.train[i] = computeCost(theta, X, y, 0)
error.valid[i] = computeCost(theta, Xval, yval, 0)
}
list(lambda_vec=lambda_vec, error.train=error.train, error.valid=error.valid)
}
list[lambda_vec, error.train, error.valid] <- validationCurve(X_poly, y, X_poly_val, yval, 1000)
df_cv <- data.frame(lambda_vec=lambda_vec, error.train=error.train, error.valid=error.valid)
ggplot(df_cv, aes(lambda_vec)) + geom_line(aes(y=error.train, col="training error")) +
geom_line(aes(y=error.valid, col="validation error")) +
labs(x="Lambda", y="Error") +
scale_color_discrete(name="")
validationCurve <- function(X, y, Xval, yval, maxit) {
lambda_vec = c(0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10)
m = length(lambda_vec)
error.train = matrix(0, m, 1)
error.valid = matrix(0, m, 1)
for(i in 1:m) {
lambda = lambda_vec[i]
theta <- trainLinearReg(X, y, lambda, maxit)
error.train[i] = computeCost(theta, X, y, 0)
error.valid[i] = computeCost(theta, Xval, yval, 0)
}
list(lambda_vec=lambda_vec, error.train=error.train, error.valid=error.valid)
}
list[lambda_vec, error.train, error.valid] <- validationCurve(X_poly, y, X_poly_val, yval, 200)
df_cv <- data.frame(lambda_vec=lambda_vec, error.train=error.train, error.valid=error.valid)
ggplot(df_cv, aes(lambda_vec)) + geom_line(aes(y=error.train, col="training error")) +
geom_line(aes(y=error.valid, col="validation error")) +
labs(x="Lambda", y="Error") +
scale_color_discrete(name="")
error.train
train.valid
error.valid
